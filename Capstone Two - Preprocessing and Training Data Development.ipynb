{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e19ae7-51d1-4bbe-9aba-a3358471bdcc",
   "metadata": {},
   "source": [
    "### In my EDA notebook, I essentially performed standardization of my data by computing the normalization by the z-score.  I will move onto creating dummy features from the state/territory series, and splitting the data into testing and training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "994da70d-3e7e-4c9b-97f2-d2e5bf23ddfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe55c3-7ff5-4378-9734-f4523a0bfef2",
   "metadata": {},
   "source": [
    "#### First I import the packages used to load the csv and create the dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e66a550-5abe-48fc-836b-cac2c83f8e1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Institution_ID', 'institution_name', 'city',\n",
       "       'state_or_territory_abbreviation', 'predominant_degree_awarded',\n",
       "       'territory_or_state_area_sqmi', 'territory_or_state_population',\n",
       "       'admission_rate', 'act_25th_percentile_score', 'graduation_rate',\n",
       "       'graduation_frequency'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "institution_data = pd.read_csv('institution_data.csv')\n",
    "institution_data = institution_data.drop(\"Unnamed: 0\",axis=1)\n",
    "institution_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710c8aed-eba9-47c2-93a7-9d97510401a7",
   "metadata": {},
   "source": [
    "#### The dataset looks good, I've removed that annoying \"Unnamed: 0\" series that mirrors the index value for each record.  Next I'll use pandas get_dummies on series \"state_or_territory_abbreviation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6eebb2c-360e-4871-a535-181c77db5c12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Institution_ID</th>\n",
       "      <th>territory_or_state_area_sqmi</th>\n",
       "      <th>territory_or_state_population</th>\n",
       "      <th>admission_rate</th>\n",
       "      <th>act_25th_percentile_score</th>\n",
       "      <th>graduation_rate</th>\n",
       "      <th>graduation_frequency</th>\n",
       "      <th>AL</th>\n",
       "      <th>AR</th>\n",
       "      <th>AZ</th>\n",
       "      <th>...</th>\n",
       "      <th>TN</th>\n",
       "      <th>TX</th>\n",
       "      <th>UT</th>\n",
       "      <th>VA</th>\n",
       "      <th>VI</th>\n",
       "      <th>VT</th>\n",
       "      <th>WA</th>\n",
       "      <th>WI</th>\n",
       "      <th>WV</th>\n",
       "      <th>WY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100654</td>\n",
       "      <td>-0.329042</td>\n",
       "      <td>-0.654946</td>\n",
       "      <td>-0.007511</td>\n",
       "      <td>-1.249341</td>\n",
       "      <td>-1.796206</td>\n",
       "      <td>-0.207178</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100663</td>\n",
       "      <td>-0.329042</td>\n",
       "      <td>-0.654946</td>\n",
       "      <td>-0.260956</td>\n",
       "      <td>0.408841</td>\n",
       "      <td>-0.354295</td>\n",
       "      <td>0.494820</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100706</td>\n",
       "      <td>-0.329042</td>\n",
       "      <td>-0.654946</td>\n",
       "      <td>0.802489</td>\n",
       "      <td>0.961569</td>\n",
       "      <td>-0.984914</td>\n",
       "      <td>-0.317649</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100724</td>\n",
       "      <td>-0.329042</td>\n",
       "      <td>-0.654946</td>\n",
       "      <td>-0.979817</td>\n",
       "      <td>-1.249341</td>\n",
       "      <td>-1.588804</td>\n",
       "      <td>0.325588</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100751</td>\n",
       "      <td>-0.329042</td>\n",
       "      <td>-0.654946</td>\n",
       "      <td>-0.611170</td>\n",
       "      <td>0.408841</td>\n",
       "      <td>0.427793</td>\n",
       "      <td>4.138001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Institution_ID  territory_or_state_area_sqmi  \\\n",
       "0          100654                     -0.329042   \n",
       "1          100663                     -0.329042   \n",
       "2          100706                     -0.329042   \n",
       "3          100724                     -0.329042   \n",
       "4          100751                     -0.329042   \n",
       "\n",
       "   territory_or_state_population  admission_rate  act_25th_percentile_score  \\\n",
       "0                      -0.654946       -0.007511                  -1.249341   \n",
       "1                      -0.654946       -0.260956                   0.408841   \n",
       "2                      -0.654946        0.802489                   0.961569   \n",
       "3                      -0.654946       -0.979817                  -1.249341   \n",
       "4                      -0.654946       -0.611170                   0.408841   \n",
       "\n",
       "   graduation_rate  graduation_frequency  AL  AR  AZ  ...  TN  TX  UT  VA  VI  \\\n",
       "0        -1.796206             -0.207178   1   0   0  ...   0   0   0   0   0   \n",
       "1        -0.354295              0.494820   1   0   0  ...   0   0   0   0   0   \n",
       "2        -0.984914             -0.317649   1   0   0  ...   0   0   0   0   0   \n",
       "3        -1.588804              0.325588   1   0   0  ...   0   0   0   0   0   \n",
       "4         0.427793              4.138001   1   0   0  ...   0   0   0   0   0   \n",
       "\n",
       "   VT  WA  WI  WV  WY  \n",
       "0   0   0   0   0   0  \n",
       "1   0   0   0   0   0  \n",
       "2   0   0   0   0   0  \n",
       "3   0   0   0   0   0  \n",
       "4   0   0   0   0   0  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "institution_data_dummies = pd.get_dummies(institution_data, columns=['state_or_territory_abbreviation'], prefix='', prefix_sep='', drop_first=True)\n",
    "institution_data_dummies = institution_data_dummies.drop(columns = ['institution_name','city','predominant_degree_awarded'])\n",
    "institution_data_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da957839-3aa4-4510-86d8-c4783feeed23",
   "metadata": {},
   "source": [
    "#### The above head of shows the dummy features included on the far right of the dataframe.  I removed the prefix and prefix seperator to make the feature names easier to read.  Now I need to split the data into training and test sets.  My independent variables include all features that are not graduation_rate.  The dependent variable is the graduation_rate series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "083a35ac-9b9a-4da3-b028-65e88906d317",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(institution_data_dummies.drop(columns='graduation_rate'), institution_data_dummies.graduation_rate, test_size = .30, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b07753d0-016e-4a9d-bcd8-441c50e41789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train.csv')\n",
    "X_test.to_csv('X_test.csv')\n",
    "y_train.to_csv('y_train.csv')\n",
    "y_test.to_csv('y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af90543-b679-4b20-ad8b-1a84502439e2",
   "metadata": {},
   "source": [
    "#### I exported my training and test dataframes for the future model notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
